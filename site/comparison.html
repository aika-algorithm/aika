<!DOCTYPE HTML>
<html>
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AIKA vs. PyTorch: A Comparison</title>
    <link rel="stylesheet" href="css/style.css" neuronType="text/css">
    <link rel="shortcut icon" href="images/favicon.png" />
</head>
<body>
<div id="header">
    <div>
        <div class="logo">
            <a rel="canonical" href="https://aika.network"></a>
        </div>
        <ul id="navigation">
            <li>
                <a href="index.html">Overall Idea</a>
            </li>
            <li>
                <a href="blog.html">Blog</a>
            </li>
            <li>
                <a href="usage.html">Examples</a>
            </li>
            <li>
                <a href="installation.html">Installation</a>
            </li>
            <li class="active">
                <a href="comparison.html">Comparison</a>
            </li>
            <li>
                <a href="resources.html">Resources</a>
            </li>
            <li>
                <a href="https://github.com/aika-algorithm/aika-cpp">GitHub</a>
            </li>
        </ul>
    </div>
</div>
<div id="contents">
    <div id="features">
        <h1>AIKA vs. PyTorch: A Comparison</h1>
        <p>
            AIKA takes a fundamentally different approach compared to traditional deep learning frameworks like PyTorch. Like PyTorch, AIKA combines Python for model definition with high-performance C++ for execution, but the underlying philosophy and execution model are quite different.
        </p>

        <h2>Architecture Comparison</h2>
        <p>
            Both AIKA and PyTorch use a <strong>Python frontend with C++ backend</strong> architecture, but their approaches diverge significantly:
        </p>
        <table border="1" cellspacing="0" cellpadding="5">
            <tr>
                <th>Aspect</th>
                <th>AIKA (C++ + Python)</th>
                <th>PyTorch (C++ + Python)</th>
            </tr>
            <tr>
                <td><b>Backend</b></td>
                <td>Custom C++20 core with pybind11 bindings</td>
                <td>libtorch (C++) with Python bindings</td>
            </tr>
            <tr>
                <td><b>Data Model</b></td>
                <td>Dynamically instantiated object graph</td>
                <td>Predefined tensor dimensions</td>
            </tr>
            <tr>
                <td><b>Computation Model</b></td>
                <td>Event-driven processing via activation queue</td>
                <td>Batch-based matrix operations</td>
            </tr>
            <tr>
                <td><b>Processing</b></td>
                <td>Asynchronous, sparse activations</td>
                <td>Synchronous, dense computations</td>
            </tr>
            <tr>
                <td><b>Architecture</b></td>
                <td>Type-hierarchy defining neural elements</td>
                <td>Layered networks using tensor algebra</td>
            </tr>
            <tr>
                <td><b>Flexibility</b></td>
                <td>Neurons and connections instantiated at runtime</td>
                <td>Fixed-size tensors and layers</td>
            </tr>
            <tr>
                <td><b>Use Case</b></td>
                <td>Research, interpretable AI, dynamic structures</td>
                <td>Production, high-throughput numerical computation</td>
            </tr>
        </table>

        <h2>Key Differences</h2>

        <h3>1. Dynamic Object Graph vs. Static Tensor Ops</h3>
        <p>
            <strong>PyTorch:</strong> Uses predefined tensor operations (linear layers, conv layers, etc.) on fixed-size vectors/matrices. Network structure is defined upfront with set dimensions.
        </p>
        <p>
            <strong>AIKA:</strong> Builds computations through dynamically instantiated objects derived from a type hierarchy. Networks can spawn new neuron or activation instances on the fly, allowing variable structures and sparsely populated data to be handled naturally.
        </p>

        <h3>2. Event-Driven Asynchrony vs. Synchronous Execution</h3>
        <p>
            <strong>PyTorch:</strong> Computations proceed synchronously (forward pass, then backward pass, typically on batched data).
        </p>
        <p>
            <strong>AIKA:</strong> Employs an event-driven mechanism with lexicographic queue ordering: <code>(round, phase, -priority, timestamp)</code>. When certain conditions are met (e.g., a neuron's threshold is exceeded), an event is queued and processed in order. Different parts of the network can update at different times, efficiently handling sparse or sequentially dependent activations.
        </p>

        <h3>3. Flexible Topology vs. Layered Architecture</h3>
        <p>
            <strong>PyTorch:</strong> Models are usually defined as a stack of layers or modules that data flows through. The computation graph is generally fixed once the model is defined (aside from control-flow logic in dynamic graphs).
        </p>
        <p>
            <strong>AIKA:</strong> Does away with a strictly layered design. The functional graph is tied to objects created during runtime. Because of the type hierarchy and dynamic instantiation, the network topology can be more flexible and adaptive—the structure can change depending on the input and the events triggered.
        </p>

        <h2>Advantages of AIKA</h2>
        <ul>
            <li><b>Sparse & Efficient:</b> AIKA only activates relevant parts of the network, reducing computation time for sparse data.</li>
            <li><b>More Interpretable:</b> Individual activations are objects that can be traced back to symbolic representations, making the network more transparent.</li>
            <li><b>Flexible Topology:</b> The network structure evolves dynamically rather than being fixed, enabling adaptive architectures.</li>
            <li><b>Type-Based Design:</b> The type hierarchy provides better organization and reuse when building complex models.</li>
            <li><b>C++ Performance:</b> High-performance C++20 core with modern design patterns (builder pattern, smart pointers, etc.).</li>
        </ul>

        <h2>Current Implementation Status</h2>
        <p>
            <strong>Maturity Level:</strong> Beta (Active Development, ~72% complete)
        </p>
        <ul>
            <li>✅ <strong>Fields Module:</strong> 100% complete and production-ready</li>
            <li>⚠️ <strong>Network Module:</strong> 85% complete (core functionality working)</li>
            <li>⚠️ <strong>Transformer:</strong> 60% complete (type structure done, attention mechanism in progress)</li>
        </ul>
        <p>
            PyTorch is a mature, production-ready framework optimized for high-throughput numerical computation with static graphs and batched data. AIKA, by contrast, explores a more <strong>adaptive, event-driven paradigm</strong> where the focus is on individual activations and their interactions. AIKA's approach may be advantageous for research into neural architectures that require dynamic structure or need to capture fine-grained causality and interactions not easily represented in matrix form.
        </p>

        <h2>When to Use AIKA</h2>
        <p>
            AIKA is particularly relevant for:
        </p>
        <ul>
            <li>AI researchers exploring alternatives to conventional deep learning</li>
            <li>Applications requiring interpretable neural network behavior</li>
            <li>Dynamic graph structures that change based on input</li>
            <li>Sparse activation patterns where only relevant neurons activate</li>
            <li>Research into event-driven or symbolic reasoning combined with neural computation</li>
        </ul>

        <h2>Getting Started</h2>
        <p>
            To get started with AIKA, visit the <a href="installation.html">Installation Guide</a> and explore the <a href="usage.html">Usage Examples</a>.
        </p>
        <p>
            For detailed technical specifications, see the <a href="https://github.com/aika-algorithm/aika-cpp/tree/main/specs">specs/ directory</a> on GitHub.
        </p>
    </div>
</div>
</body>
</html>